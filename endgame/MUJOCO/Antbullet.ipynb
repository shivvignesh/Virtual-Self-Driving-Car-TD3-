{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Antbullet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "outputId": "d56630ee-f813-4739-9642-fbb918f08a3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/ac/a422ab8d1c57ab3f43e573b5a5f532e6afd348d81308fe66a1ecb691548e/pybullet-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (95.0MB)\n",
            "\u001b[K     |████████████████████████████████| 95.0MB 88kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-2.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## Step 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size\n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## Step 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## Step 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        " \n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        " \n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" \n",
        "seed = 0 \n",
        "start_timesteps = 1e4\n",
        "eval_freq = 5e3 \n",
        "max_timesteps = 5e5 \n",
        "save_models = True \n",
        "expl_noise = 0.1 \n",
        "batch_size = 100 \n",
        "discount = 0.99 \n",
        "tau = 0.005 \n",
        "policy_noise = 0.2\n",
        "noise_clip = 0.5 \n",
        "policy_freq = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "outputId": "46e49810-26ac-415b-b538-d9b6418fe471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "outputId": "5713726b-3f3b-4cc9-c27b-adda1bafa7d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "outputId": "a670d18f-7b1e-4c9a-fd61-57af251ba4b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804960\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "outputId": "182b7798-34dd-4ea7-d728-3f02bfc3d870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_timesteps = 500000\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  if done:\n",
        "\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    obs = env.reset()\n",
        "    \n",
        "   \n",
        "    done = False\n",
        "    \n",
        " \n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: \n",
        "    action = policy.select_action(np.array(obs))\n",
        "   \n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  episode_reward += reward\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 485.81583723249526\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 355.2292736322347\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 366.5575689123727\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: 502.6088043317875\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: 515.0072392013919\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 95.617802\n",
            "---------------------------------------\n",
            "Total Timesteps: 6000 Episode Num: 6 Reward: 490.4483961824056\n",
            "Total Timesteps: 7000 Episode Num: 7 Reward: 482.4564666165142\n",
            "Total Timesteps: 8000 Episode Num: 8 Reward: 487.1176405292076\n",
            "Total Timesteps: 8231 Episode Num: 9 Reward: 110.18245529545526\n",
            "Total Timesteps: 8605 Episode Num: 10 Reward: 186.17792711648949\n",
            "Total Timesteps: 9605 Episode Num: 11 Reward: 517.6647130270798\n",
            "Total Timesteps: 10571 Episode Num: 12 Reward: 440.80841538807044\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 160.277686\n",
            "---------------------------------------\n",
            "Total Timesteps: 11571 Episode Num: 13 Reward: 97.1333467054353\n",
            "Total Timesteps: 11678 Episode Num: 14 Reward: -26.840684036135755\n",
            "Total Timesteps: 12678 Episode Num: 15 Reward: 79.31754188065094\n",
            "Total Timesteps: 13678 Episode Num: 16 Reward: 240.89167067806122\n",
            "Total Timesteps: 13793 Episode Num: 17 Reward: -6.025675398152732\n",
            "Total Timesteps: 13913 Episode Num: 18 Reward: -3.979712119710824\n",
            "Total Timesteps: 14055 Episode Num: 19 Reward: 9.476085373174326\n",
            "Total Timesteps: 15055 Episode Num: 20 Reward: 112.64963354721205\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 51.434083\n",
            "---------------------------------------\n",
            "Total Timesteps: 16055 Episode Num: 21 Reward: 137.92865502751195\n",
            "Total Timesteps: 17055 Episode Num: 22 Reward: 185.410314821075\n",
            "Total Timesteps: 18055 Episode Num: 23 Reward: 85.42624549148474\n",
            "Total Timesteps: 19055 Episode Num: 24 Reward: 226.0133331928944\n",
            "Total Timesteps: 20055 Episode Num: 25 Reward: 113.18652514997672\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 252.185420\n",
            "---------------------------------------\n",
            "Total Timesteps: 21055 Episode Num: 26 Reward: 287.68790431314915\n",
            "Total Timesteps: 22055 Episode Num: 27 Reward: 282.0004929856381\n",
            "Total Timesteps: 23055 Episode Num: 28 Reward: 302.99930691332145\n",
            "Total Timesteps: 24055 Episode Num: 29 Reward: 390.9312988094534\n",
            "Total Timesteps: 25055 Episode Num: 30 Reward: 295.6487996607183\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 288.441000\n",
            "---------------------------------------\n",
            "Total Timesteps: 26055 Episode Num: 31 Reward: 304.58576622975676\n",
            "Total Timesteps: 27055 Episode Num: 32 Reward: 210.64975360738921\n",
            "Total Timesteps: 28055 Episode Num: 33 Reward: 105.08409259108258\n",
            "Total Timesteps: 29055 Episode Num: 34 Reward: 405.82202684318463\n",
            "Total Timesteps: 30055 Episode Num: 35 Reward: 260.79095791119846\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 147.751199\n",
            "---------------------------------------\n",
            "Total Timesteps: 31055 Episode Num: 36 Reward: 117.54949657960618\n",
            "Total Timesteps: 32055 Episode Num: 37 Reward: 116.7412135673138\n",
            "Total Timesteps: 33055 Episode Num: 38 Reward: 279.3425196162917\n",
            "Total Timesteps: 34055 Episode Num: 39 Reward: 312.9398240859046\n",
            "Total Timesteps: 35055 Episode Num: 40 Reward: 386.4638903903486\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 253.013113\n",
            "---------------------------------------\n",
            "Total Timesteps: 36055 Episode Num: 41 Reward: 355.7196513305311\n",
            "Total Timesteps: 37055 Episode Num: 42 Reward: 101.52096548152282\n",
            "Total Timesteps: 38055 Episode Num: 43 Reward: 222.32259482781805\n",
            "Total Timesteps: 39055 Episode Num: 44 Reward: 266.7880349268728\n",
            "Total Timesteps: 40055 Episode Num: 45 Reward: 245.27488215113377\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 205.773667\n",
            "---------------------------------------\n",
            "Total Timesteps: 41055 Episode Num: 46 Reward: 206.3376557457135\n",
            "Total Timesteps: 41382 Episode Num: 47 Reward: 84.97173903082017\n",
            "Total Timesteps: 42382 Episode Num: 48 Reward: 138.46533718970957\n",
            "Total Timesteps: 43382 Episode Num: 49 Reward: 234.27253167047306\n",
            "Total Timesteps: 44382 Episode Num: 50 Reward: 406.3365048658043\n",
            "Total Timesteps: 45382 Episode Num: 51 Reward: 191.1133509143288\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 329.615731\n",
            "---------------------------------------\n",
            "Total Timesteps: 46382 Episode Num: 52 Reward: 288.5782503480567\n",
            "Total Timesteps: 47382 Episode Num: 53 Reward: 126.79532617604573\n",
            "Total Timesteps: 48382 Episode Num: 54 Reward: 381.18610075403495\n",
            "Total Timesteps: 49382 Episode Num: 55 Reward: 482.7618272212661\n",
            "Total Timesteps: 49822 Episode Num: 56 Reward: 197.92796573541773\n",
            "Total Timesteps: 50822 Episode Num: 57 Reward: 223.35635344897256\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1.910533\n",
            "---------------------------------------\n",
            "Total Timesteps: 50842 Episode Num: 58 Reward: 2.108897600247584\n",
            "Total Timesteps: 50862 Episode Num: 59 Reward: 1.8810809651022775\n",
            "Total Timesteps: 50882 Episode Num: 60 Reward: 2.9884180364972526\n",
            "Total Timesteps: 50902 Episode Num: 61 Reward: 2.0386654461763047\n",
            "Total Timesteps: 50922 Episode Num: 62 Reward: 1.3883930733480025\n",
            "Total Timesteps: 50942 Episode Num: 63 Reward: 2.2444206332444008\n",
            "Total Timesteps: 50962 Episode Num: 64 Reward: 1.2007694481746825\n",
            "Total Timesteps: 50982 Episode Num: 65 Reward: 3.2074395273053327\n",
            "Total Timesteps: 51002 Episode Num: 66 Reward: 3.4231218102852496\n",
            "Total Timesteps: 51022 Episode Num: 67 Reward: 2.8735106100211354\n",
            "Total Timesteps: 51042 Episode Num: 68 Reward: 4.891899265731446\n",
            "Total Timesteps: 51062 Episode Num: 69 Reward: 3.4510978479234162\n",
            "Total Timesteps: 51082 Episode Num: 70 Reward: 3.497266109486101\n",
            "Total Timesteps: 52082 Episode Num: 71 Reward: 563.6557563084368\n",
            "Total Timesteps: 53082 Episode Num: 72 Reward: 540.8515520086589\n",
            "Total Timesteps: 54082 Episode Num: 73 Reward: 335.06632859217797\n",
            "Total Timesteps: 55082 Episode Num: 74 Reward: 239.64606423736393\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 402.321493\n",
            "---------------------------------------\n",
            "Total Timesteps: 56082 Episode Num: 75 Reward: 476.41019289142594\n",
            "Total Timesteps: 57082 Episode Num: 76 Reward: 364.3413486107869\n",
            "Total Timesteps: 58082 Episode Num: 77 Reward: 428.21848432172163\n",
            "Total Timesteps: 59082 Episode Num: 78 Reward: 300.0685179317242\n",
            "Total Timesteps: 60082 Episode Num: 79 Reward: 376.88380961672834\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 256.806879\n",
            "---------------------------------------\n",
            "Total Timesteps: 61082 Episode Num: 80 Reward: 104.09049659506738\n",
            "Total Timesteps: 62082 Episode Num: 81 Reward: 289.63555691088726\n",
            "Total Timesteps: 63082 Episode Num: 82 Reward: 539.0330415878368\n",
            "Total Timesteps: 64082 Episode Num: 83 Reward: 452.75932602916606\n",
            "Total Timesteps: 65082 Episode Num: 84 Reward: 336.8615401342485\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 553.403295\n",
            "---------------------------------------\n",
            "Total Timesteps: 66082 Episode Num: 85 Reward: 463.9932111071017\n",
            "Total Timesteps: 67082 Episode Num: 86 Reward: 323.0133107985912\n",
            "Total Timesteps: 68082 Episode Num: 87 Reward: 480.1509015796974\n",
            "Total Timesteps: 69082 Episode Num: 88 Reward: 569.3308434465398\n",
            "Total Timesteps: 70082 Episode Num: 89 Reward: 308.92031639761274\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 603.369183\n",
            "---------------------------------------\n",
            "Total Timesteps: 71082 Episode Num: 90 Reward: 657.2726263079952\n",
            "Total Timesteps: 72082 Episode Num: 91 Reward: 310.93470666048677\n",
            "Total Timesteps: 73082 Episode Num: 92 Reward: 634.077850453977\n",
            "Total Timesteps: 74082 Episode Num: 93 Reward: 554.6047636495002\n",
            "Total Timesteps: 75082 Episode Num: 94 Reward: 535.8650450030151\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 464.471690\n",
            "---------------------------------------\n",
            "Total Timesteps: 76082 Episode Num: 95 Reward: 421.9057487738674\n",
            "Total Timesteps: 77082 Episode Num: 96 Reward: 500.9129301122155\n",
            "Total Timesteps: 78082 Episode Num: 97 Reward: 286.4001875191837\n",
            "Total Timesteps: 79082 Episode Num: 98 Reward: 465.8801528199693\n",
            "Total Timesteps: 80082 Episode Num: 99 Reward: 324.44624442193225\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 301.153128\n",
            "---------------------------------------\n",
            "Total Timesteps: 81082 Episode Num: 100 Reward: 437.45880651064766\n",
            "Total Timesteps: 82082 Episode Num: 101 Reward: 431.59879930840157\n",
            "Total Timesteps: 83082 Episode Num: 102 Reward: 272.32011749685466\n",
            "Total Timesteps: 84082 Episode Num: 103 Reward: 244.040031390974\n",
            "Total Timesteps: 85082 Episode Num: 104 Reward: 535.9232673884977\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 661.530075\n",
            "---------------------------------------\n",
            "Total Timesteps: 86082 Episode Num: 105 Reward: 694.230627814547\n",
            "Total Timesteps: 87082 Episode Num: 106 Reward: 562.2429652265043\n",
            "Total Timesteps: 88082 Episode Num: 107 Reward: 234.74859106892742\n",
            "Total Timesteps: 89082 Episode Num: 108 Reward: 238.7668606171554\n",
            "Total Timesteps: 90082 Episode Num: 109 Reward: 523.1381029236921\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 505.348698\n",
            "---------------------------------------\n",
            "Total Timesteps: 91082 Episode Num: 110 Reward: 445.2617835048276\n",
            "Total Timesteps: 92082 Episode Num: 111 Reward: 490.63204038681266\n",
            "Total Timesteps: 93082 Episode Num: 112 Reward: 593.3910414712082\n",
            "Total Timesteps: 94082 Episode Num: 113 Reward: 601.1271070495003\n",
            "Total Timesteps: 95082 Episode Num: 114 Reward: 642.6097060877523\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 514.729806\n",
            "---------------------------------------\n",
            "Total Timesteps: 96082 Episode Num: 115 Reward: 451.26709792779945\n",
            "Total Timesteps: 96724 Episode Num: 116 Reward: 251.15870308806245\n",
            "Total Timesteps: 97724 Episode Num: 117 Reward: 556.7293193874102\n",
            "Total Timesteps: 98724 Episode Num: 118 Reward: 421.18264235140396\n",
            "Total Timesteps: 99724 Episode Num: 119 Reward: 721.0781433044037\n",
            "Total Timesteps: 100724 Episode Num: 120 Reward: 494.17385930655564\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 616.685888\n",
            "---------------------------------------\n",
            "Total Timesteps: 101724 Episode Num: 121 Reward: 544.8480541947501\n",
            "Total Timesteps: 102724 Episode Num: 122 Reward: 379.6656058690569\n",
            "Total Timesteps: 103724 Episode Num: 123 Reward: 434.333187563908\n",
            "Total Timesteps: 104724 Episode Num: 124 Reward: 393.2405911512155\n",
            "Total Timesteps: 105724 Episode Num: 125 Reward: 539.388447272548\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 530.424974\n",
            "---------------------------------------\n",
            "Total Timesteps: 106724 Episode Num: 126 Reward: 350.8639274998882\n",
            "Total Timesteps: 107724 Episode Num: 127 Reward: 402.73896614645156\n",
            "Total Timesteps: 108724 Episode Num: 128 Reward: 439.72425108608616\n",
            "Total Timesteps: 109724 Episode Num: 129 Reward: 494.7748877086175\n",
            "Total Timesteps: 110724 Episode Num: 130 Reward: 705.9269442013847\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 507.694680\n",
            "---------------------------------------\n",
            "Total Timesteps: 111724 Episode Num: 131 Reward: 536.9005257346122\n",
            "Total Timesteps: 112724 Episode Num: 132 Reward: 559.3305247660458\n",
            "Total Timesteps: 113724 Episode Num: 133 Reward: 685.1547926312727\n",
            "Total Timesteps: 114724 Episode Num: 134 Reward: 406.8371261488087\n",
            "Total Timesteps: 114915 Episode Num: 135 Reward: 97.20277677694239\n",
            "Total Timesteps: 115915 Episode Num: 136 Reward: 319.5160979194182\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 534.269556\n",
            "---------------------------------------\n",
            "Total Timesteps: 116915 Episode Num: 137 Reward: 692.7105596757249\n",
            "Total Timesteps: 117915 Episode Num: 138 Reward: 326.36955327600583\n",
            "Total Timesteps: 118915 Episode Num: 139 Reward: 623.9422999169306\n",
            "Total Timesteps: 119915 Episode Num: 140 Reward: 547.2154899713842\n",
            "Total Timesteps: 120915 Episode Num: 141 Reward: 310.7076580935679\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 615.913159\n",
            "---------------------------------------\n",
            "Total Timesteps: 121915 Episode Num: 142 Reward: 576.3434249989982\n",
            "Total Timesteps: 122915 Episode Num: 143 Reward: 554.2705013945877\n",
            "Total Timesteps: 123915 Episode Num: 144 Reward: 469.88261746660817\n",
            "Total Timesteps: 124915 Episode Num: 145 Reward: 681.9097101900783\n",
            "Total Timesteps: 125915 Episode Num: 146 Reward: 625.754092937077\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 513.441488\n",
            "---------------------------------------\n",
            "Total Timesteps: 126915 Episode Num: 147 Reward: 657.4012409911944\n",
            "Total Timesteps: 127915 Episode Num: 148 Reward: 600.2088036492803\n",
            "Total Timesteps: 128915 Episode Num: 149 Reward: 535.8706826425179\n",
            "Total Timesteps: 129915 Episode Num: 150 Reward: 494.25979354687325\n",
            "Total Timesteps: 130915 Episode Num: 151 Reward: 510.51783269615316\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 655.257429\n",
            "---------------------------------------\n",
            "Total Timesteps: 131915 Episode Num: 152 Reward: 763.7462888664714\n",
            "Total Timesteps: 132915 Episode Num: 153 Reward: 672.9851320519464\n",
            "Total Timesteps: 133915 Episode Num: 154 Reward: 564.7504359703171\n",
            "Total Timesteps: 134915 Episode Num: 155 Reward: 536.3461105443997\n",
            "Total Timesteps: 135915 Episode Num: 156 Reward: 701.1236905854087\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 586.251079\n",
            "---------------------------------------\n",
            "Total Timesteps: 136915 Episode Num: 157 Reward: 646.5112676327001\n",
            "Total Timesteps: 137915 Episode Num: 158 Reward: 510.6790283241052\n",
            "Total Timesteps: 138915 Episode Num: 159 Reward: 518.8915384984845\n",
            "Total Timesteps: 139915 Episode Num: 160 Reward: 502.55741363084843\n",
            "Total Timesteps: 140915 Episode Num: 161 Reward: 711.4451564693715\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 443.329995\n",
            "---------------------------------------\n",
            "Total Timesteps: 141915 Episode Num: 162 Reward: 321.6158961626243\n",
            "Total Timesteps: 142915 Episode Num: 163 Reward: 218.6582368421117\n",
            "Total Timesteps: 143915 Episode Num: 164 Reward: 97.01777854085157\n",
            "Total Timesteps: 144915 Episode Num: 165 Reward: 352.64646921383985\n",
            "Total Timesteps: 145915 Episode Num: 166 Reward: 672.8848751123326\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 537.850339\n",
            "---------------------------------------\n",
            "Total Timesteps: 145935 Episode Num: 167 Reward: 3.555776906016407\n",
            "Total Timesteps: 146935 Episode Num: 168 Reward: 881.1393441659467\n",
            "Total Timesteps: 147935 Episode Num: 169 Reward: 722.277684652236\n",
            "Total Timesteps: 148935 Episode Num: 170 Reward: 370.6258063620295\n",
            "Total Timesteps: 149935 Episode Num: 171 Reward: 342.61894717393807\n",
            "Total Timesteps: 150935 Episode Num: 172 Reward: 526.1162434253047\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 508.252993\n",
            "---------------------------------------\n",
            "Total Timesteps: 151935 Episode Num: 173 Reward: 474.51755087886716\n",
            "Total Timesteps: 152935 Episode Num: 174 Reward: 596.8939941013775\n",
            "Total Timesteps: 153935 Episode Num: 175 Reward: 646.196750217401\n",
            "Total Timesteps: 154935 Episode Num: 176 Reward: 680.3227814011459\n",
            "Total Timesteps: 155935 Episode Num: 177 Reward: 664.7571904490566\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 577.624202\n",
            "---------------------------------------\n",
            "Total Timesteps: 156935 Episode Num: 178 Reward: 600.4350698281879\n",
            "Total Timesteps: 157935 Episode Num: 179 Reward: 393.8066720998626\n",
            "Total Timesteps: 158935 Episode Num: 180 Reward: 555.7338801253427\n",
            "Total Timesteps: 159935 Episode Num: 181 Reward: 640.7070374504596\n",
            "Total Timesteps: 160935 Episode Num: 182 Reward: 497.1014147601885\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 671.126517\n",
            "---------------------------------------\n",
            "Total Timesteps: 161935 Episode Num: 183 Reward: 560.9165919359535\n",
            "Total Timesteps: 162935 Episode Num: 184 Reward: 709.3253890958489\n",
            "Total Timesteps: 163935 Episode Num: 185 Reward: 751.8238330227135\n",
            "Total Timesteps: 164935 Episode Num: 186 Reward: 590.424503921322\n",
            "Total Timesteps: 165935 Episode Num: 187 Reward: 531.2768613612378\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 541.919313\n",
            "---------------------------------------\n",
            "Total Timesteps: 166935 Episode Num: 188 Reward: 540.652565176893\n",
            "Total Timesteps: 167935 Episode Num: 189 Reward: 484.5880221880677\n",
            "Total Timesteps: 168935 Episode Num: 190 Reward: 767.7813468409539\n",
            "Total Timesteps: 169935 Episode Num: 191 Reward: 404.2787573333798\n",
            "Total Timesteps: 170935 Episode Num: 192 Reward: 696.0467288679155\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 576.375799\n",
            "---------------------------------------\n",
            "Total Timesteps: 171935 Episode Num: 193 Reward: 621.8544414692805\n",
            "Total Timesteps: 172935 Episode Num: 194 Reward: 728.8617617912166\n",
            "Total Timesteps: 173935 Episode Num: 195 Reward: 810.423231378759\n",
            "Total Timesteps: 174935 Episode Num: 196 Reward: 682.2141434554114\n",
            "Total Timesteps: 175935 Episode Num: 197 Reward: 649.0151651986548\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 472.253209\n",
            "---------------------------------------\n",
            "Total Timesteps: 176935 Episode Num: 198 Reward: 460.18287658236073\n",
            "Total Timesteps: 177935 Episode Num: 199 Reward: 456.0545788781174\n",
            "Total Timesteps: 178935 Episode Num: 200 Reward: 747.9510250450612\n",
            "Total Timesteps: 179935 Episode Num: 201 Reward: 612.3919942017299\n",
            "Total Timesteps: 180935 Episode Num: 202 Reward: 681.7520803985784\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 721.115293\n",
            "---------------------------------------\n",
            "Total Timesteps: 181935 Episode Num: 203 Reward: 688.0838511954581\n",
            "Total Timesteps: 182935 Episode Num: 204 Reward: 643.5454326230363\n",
            "Total Timesteps: 183935 Episode Num: 205 Reward: 411.5463430436239\n",
            "Total Timesteps: 184935 Episode Num: 206 Reward: 676.0719568421937\n",
            "Total Timesteps: 185935 Episode Num: 207 Reward: 685.5421737054596\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 662.107917\n",
            "---------------------------------------\n",
            "Total Timesteps: 186935 Episode Num: 208 Reward: 543.6764394259734\n",
            "Total Timesteps: 187935 Episode Num: 209 Reward: 661.5855452901469\n",
            "Total Timesteps: 188935 Episode Num: 210 Reward: 560.7183636066894\n",
            "Total Timesteps: 189935 Episode Num: 211 Reward: 674.9800327218891\n",
            "Total Timesteps: 190935 Episode Num: 212 Reward: 303.54134436287274\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 514.966558\n",
            "---------------------------------------\n",
            "Total Timesteps: 191935 Episode Num: 213 Reward: 687.0927782382871\n",
            "Total Timesteps: 192935 Episode Num: 214 Reward: 416.588567316877\n",
            "Total Timesteps: 193935 Episode Num: 215 Reward: 682.6650338799183\n",
            "Total Timesteps: 194935 Episode Num: 216 Reward: 669.5071767231777\n",
            "Total Timesteps: 195935 Episode Num: 217 Reward: 529.8581011036949\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 749.390092\n",
            "---------------------------------------\n",
            "Total Timesteps: 196935 Episode Num: 218 Reward: 642.2660175819508\n",
            "Total Timesteps: 197935 Episode Num: 219 Reward: 708.5512552920592\n",
            "Total Timesteps: 198935 Episode Num: 220 Reward: 957.4446696365171\n",
            "Total Timesteps: 199935 Episode Num: 221 Reward: 753.8846614854181\n",
            "Total Timesteps: 200935 Episode Num: 222 Reward: 758.4722577530925\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 633.293033\n",
            "---------------------------------------\n",
            "Total Timesteps: 201935 Episode Num: 223 Reward: 589.1362443239938\n",
            "Total Timesteps: 202935 Episode Num: 224 Reward: 849.6826533636932\n",
            "Total Timesteps: 203935 Episode Num: 225 Reward: 774.1505482469745\n",
            "Total Timesteps: 204935 Episode Num: 226 Reward: 784.4507256882335\n",
            "Total Timesteps: 205935 Episode Num: 227 Reward: 482.34690510976804\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 584.303693\n",
            "---------------------------------------\n",
            "Total Timesteps: 206935 Episode Num: 228 Reward: 638.626444811666\n",
            "Total Timesteps: 207935 Episode Num: 229 Reward: 839.3859469504752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-14812cd6e887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total Timesteps: {} Episode Num: {} Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# We evaluate the episode and we save the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-da16fa8c629e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;31m# Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcnexWrW4a8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}